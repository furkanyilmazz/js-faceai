{"version":3,"sources":["App.js","reportWebVitals.js","index.js"],"names":["App","useState","initializing","setInitializing","videoRef","useRef","canvasRef","useEffect","a","MODEL_URL","Promise","all","faceapi","tinyFaceDetector","loadFromUri","faceLandmark68Net","faceRecognitionNet","faceExpressionNet","then","startVideo","loadModels","navigator","getUserMedia","webkitGetUserMedia","mozGetUserMedia","video","stream","document","querySelector","srcObject","play","err","console","log","name","className","id","ref","autoPlay","muted","height","width","onPlay","setInterval","canvas","current","body","append","displaySize","heigth","withFaceLandmarks","withFaceExpressions","detections","resizedDetections","getContext","clearRect","drawDetections","drawFaceLandmarks","drawFaceExpressions","reportWebVitals","onPerfEntry","Function","getCLS","getFID","getFCP","getLCP","getTTFB","ReactDOM","render","StrictMode","getElementById"],"mappings":"0TAKe,SAASA,IAEtB,IAF4B,EAIYC,oBAAS,GAJrB,mBAIrBC,EAJqB,KAIPC,EAJO,KAKtBC,EAAWC,mBACXC,EAAYD,mBAElBE,qBAAU,YACQ,uCAAG,4BAAAC,EAAA,sDACXC,EAAqC,UAC3CN,GAAgB,GAChBO,QAAQC,IAAI,CACVC,IAAaC,iBAAiBC,YAAYL,GAC1CG,IAAaG,kBAAkBD,YAAYL,GAC3CG,IAAaI,mBAAmBF,YAAYL,GAC5CG,IAAaK,kBAAkBH,YAAYL,KAC1CS,KAAKC,GARS,2CAAH,qDAUhBC,KACC,IAEH,IAAMD,EAAa,WACjBE,UAAUC,aAAeD,UAAUC,cACjCD,UAAUE,oBACVF,UAAUG,gBAEZH,UAAUC,aACR,CACEG,MAAO,KAET,SAAUC,GACR,IAAID,EAAQE,SAASC,cAAc,SACnCH,EAAMI,UAAYH,EAClBD,EAAMK,UAER,SAAUC,GACRC,QAAQC,IAAI,SAAWF,EAAIG,UA6BjC,OACE,sBAAKC,UAAU,OAAf,UACE,+BAAOjC,EAAe,eAAiB,UACvC,gCACE,uBAAOkC,GAAG,QAAQD,UAAU,QAAQE,IAAKjC,EAAUkC,UAAQ,EAACC,OAAK,EAACC,OApEpD,IAoEyEC,MAnE1E,KAmE6FC,OA1BtF,WACxBC,YAAW,sBAAC,kCAAAnC,EAAA,6DACNN,GACFC,GAAgB,GAEZyC,EAAShC,IAA8BR,EAASyC,SACtDlB,SAASmB,KAAKC,OAAOH,GACfI,EAAc,CAClBP,MAjDa,KAkDbQ,OAnDc,KAqDhBrC,IAAwBgC,EAAQI,GAVtB,SAWepC,IAAuBR,EAASyC,QAAS,IAAIjC,KAAmCsC,oBAAoBC,sBAXnH,OAWJC,EAXI,OAYJC,EAAoBzC,IAAsBwC,EAAYJ,GAC5DJ,EAAOU,WAAW,MAAMC,UAAU,EAAG,EAAGX,EAAOH,MAAOG,EAAOJ,QAC7D5B,IAAa4C,eAAeZ,EAAQS,GACpCzC,IAAa6C,kBAAkBb,EAAQS,GACvCzC,IAAa8C,oBAAoBd,EAAQS,GAhB/B,4CAkBT,QAQC,wBAAQhB,IAAK/B,UC5ErB,IAYeqD,EAZS,SAAAC,GAClBA,GAAeA,aAAuBC,UACxC,6BAAqB3C,MAAK,YAAkD,IAA/C4C,EAA8C,EAA9CA,OAAQC,EAAsC,EAAtCA,OAAQC,EAA8B,EAA9BA,OAAQC,EAAsB,EAAtBA,OAAQC,EAAc,EAAdA,QAC3DJ,EAAOF,GACPG,EAAOH,GACPI,EAAOJ,GACPK,EAAOL,GACPM,EAAQN,OCDdO,IAASC,OACP,cAAC,IAAMC,WAAP,UACE,cAAC,EAAD,MAEF1C,SAAS2C,eAAe,SAM1BX,M","file":"static/js/main.a934ae1e.chunk.js","sourcesContent":["import React, { useEffect, useState, useRef } from 'react';\nimport \"./App.css\";\n\nimport * as faceapi from 'face-api.js';\n\nexport default function App() {\n\n  const videoHeight = 720;\n  const videoWidth = 1280;\n  const [initializing, setInitializing] = useState(false)\n  const videoRef = useRef();\n  const canvasRef = useRef();\n\n  useEffect(() => {\n    const loadModels = async () => {\n      const MODEL_URL = process.env.PUBLIC_URL + '/models';\n      setInitializing(true);\n      Promise.all([\n        faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL),\n        faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL),\n        faceapi.nets.faceRecognitionNet.loadFromUri(MODEL_URL),\n        faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL)\n      ]).then(startVideo)\n    }\n    loadModels();\n  }, []);\n\n  const startVideo = () => {\n    navigator.getUserMedia = navigator.getUserMedia ||\n      navigator.webkitGetUserMedia ||\n      navigator.mozGetUserMedia;\n\n    navigator.getUserMedia(\n      {\n        video: {},\n      },\n      function (stream) {\n        var video = document.querySelector('video');\n        video.srcObject = stream;\n        video.play();\n      },\n      function (err) {\n        console.log(\"hata: \" + err.name);\n      }\n    );\n\n\n  }\n\n  const handleVideoOnPlay = () => {\n    setInterval(async () => {\n      if (initializing) {\n        setInitializing(false);\n      }\n      const canvas = faceapi.createCanvasFromMedia(videoRef.current);\n      document.body.append(canvas);\n      const displaySize = {\n        width: videoWidth,\n        heigth: videoHeight\n      }\n      faceapi.matchDimensions(canvas, displaySize);\n      const detections = await faceapi.detectAllFaces(videoRef.current, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceExpressions();\n      const resizedDetections = faceapi.resizeResults(detections, displaySize);\n      canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height);\n      faceapi.draw.drawDetections(canvas, resizedDetections);\n      faceapi.draw.drawFaceLandmarks(canvas, resizedDetections);\n      faceapi.draw.drawFaceExpressions(canvas, resizedDetections)\n      //console.log(detections);\n    }, 100)\n  }\n\n  return (\n    <div className=\"main\">\n      <span>{initializing ? \"Initializing\" : \"Ready\"}</span>\n      <div>\n        <video id=\"video\" className=\"video\" ref={videoRef} autoPlay muted height={videoHeight} width={videoWidth} onPlay={handleVideoOnPlay} />\n        <canvas ref={canvasRef} />\n      </div>\n    </div>\n  )\n}\n\n","const reportWebVitals = onPerfEntry => {\n  if (onPerfEntry && onPerfEntry instanceof Function) {\n    import('web-vitals').then(({ getCLS, getFID, getFCP, getLCP, getTTFB }) => {\n      getCLS(onPerfEntry);\n      getFID(onPerfEntry);\n      getFCP(onPerfEntry);\n      getLCP(onPerfEntry);\n      getTTFB(onPerfEntry);\n    });\n  }\n};\n\nexport default reportWebVitals;\n","import React from 'react';\nimport ReactDOM from 'react-dom';\nimport './index.css';\nimport App from './App';\nimport reportWebVitals from './reportWebVitals';\n\nReactDOM.render(\n  <React.StrictMode>\n    <App />\n  </React.StrictMode>,\n  document.getElementById('root')\n);\n\n// If you want to start measuring performance in your app, pass a function\n// to log results (for example: reportWebVitals(console.log))\n// or send to an analytics endpoint. Learn more: https://bit.ly/CRA-vitals\nreportWebVitals();\n"],"sourceRoot":""}